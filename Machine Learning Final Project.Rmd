---
title: "Practical Macine Learning Final Project"
author: "Kevin McCue"
date: "December 8, 2017"
output: html_document
---


Introduction:
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. 

The manner in which they did the exercise is the "classe" variable in the training set. Any of the other variables are fair game to predict with. 

##Beginning:  Loading the Required Packages:
```{r Beginning, warning = FALSE, eval= TRUE, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~")
rm(list = ls())
set.seed(1338)
if (!require("caret")) {install.packages("caret")}
library(caret)
if (!require("randomForest")) {install.packages("randomForest")}
library(randomForest)
if (!require("e1071")) {install.packages("e1071")}
library(e1071)
if (!require("plyr")) {install.packages("plyr")}
library(plyr)
if (!require("reshape2")) {install.packages("reshape2")}
library(reshape2)
if (!require("plotly")) {install.packages("plotly")}
library(plotly)
```

##List of required packages:
- caret
- randomForest
- e1071
- plyr
- reshape2
- plotly


##Loading the Data:

The first thing I do is set the working directory to default and clear the environment.  Then, I set my desired working directory and load the files or download them if they are not present in the work directory.
```{r DataLoading}
setwd("~")
rm(list = ls())

mainDir <- "~."
subDir <- "R"
subDir2 <- "Johns Hopkins Data Science Certification"
subDir3 <- "Practical Machine Learning"
if (file.exists(subDir)) { setwd(file.path(mainDir, subDir))} else {dir.create(file.path(mainDir, subDir), showWarnings = FALSE) 
setwd(file.path(mainDir, subDir))}
  mainDir <- getwd()
if (file.exists(subDir2)) { setwd(file.path(mainDir, subDir2))} else {dir.create(file.path(mainDir, subDir2), showWarnings = FALSE) 
setwd(file.path(mainDir, subDir2))}
  mainDir <- getwd()
if (file.exists(subDir3)) {setwd(file.path(mainDir, subDir3))} else {dir.create(file.path(mainDir,subDir3), showWarnings = FALSE) 
setwd(file.path(mainDir, subDir3))}
  
trainFileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainFileName <- "pml-training.csv"
  
if (!file.exists(trainFileName)) {
    download.file(trainFileURL ,trainFileName,method="auto") }
train <- read.csv(trainFileName, header = TRUE, sep = ",", quote = "\"")
  
testFileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testFileName <- "pml-testing.csv"
  
if (!file.exists(testFileName)) {
    download.file(testFileURL ,testFileName,method="auto") }
test <- read.csv(testFileName, header = TRUE, sep = ",", quote = "\"")
```

The test dataset is only 20 rows of the data, and is more of a verification dataset, but too small to really be that either.  So, the training data will be partitioned in two after cleaning and tidying the data.


##Cleaning the Data:

There are 160 columns of data to begin with.  
```{r}
dim(train)[2]
```

First, I remove the first 7 columns as they are of no use.
```{r cleaning1}
train <- subset(train, select=-c(1:7))
test <- subset(test, select=-c(1:7))
```

Second, I use a threshold value to check the number of occurences of 'NA' or '""' empty values in each column to remove columns which have more than 95 % NA or empty values.
```{r cleaning2}
train_threshold_val <- 0.95 * dim(train)[1]
include_cols <- !apply(train, 2, function(x) sum(is.na(x)) > train_threshold_val || sum(x=="") > train_threshold_val)
train <- train[, include_cols]
train_threshold_val <- 0.95 * dim(train)[1]
```

Thirdly, I remove columns with very low variance values.
```{r cleaning3}
nearZvar <- nearZeroVar(train, saveMetrics = TRUE)
train <- train[ , nearZvar$nzv==FALSE] 
```

Lastly, I remove columns highly correlated with one another via correlation matrix.
```{r cleaning4}
Corr_Matrix <- abs(cor(train[,-dim(train)[2]]))
diag(Corr_Matrix) <- 0
HighlyCorrelated_col <- findCorrelation(Corr_Matrix, verbose = FALSE , cutoff = .95)
train <- train[, -c(HighlyCorrelated_col)]
dim(train)[2]
```
So, the dataset has been reduced from 160 columns to 49 columns, and the number of row remains unchnged at 19,622.


##Partitioning the Data:

Next, I Partition the train dataset into 2 parts with p=0.66 so that there is are training and test datasets.
```{r partition}
Partition = createDataPartition(train$classe, p = 0.66, list=FALSE)
train1 <- train[Partition,]
train2 <- train[-Partition,]
```


##Random Forest Model:  

I use 'importance=TRUE' for use of the Variable-Importance plots to help interpret the model some.
```{r randomForest}
RFModel <- randomForest(classe ~., data=train1, importance=TRUE, ntree=250)
RFModel
```

A Frequency Table of the predicted values for the test set:
```{r FreqTableTrain2}
train2_pred <- predict(RFModel, newdata=train2)
train2_table <- count(train2_pred)
train2_table$Percent <- train2_table$freq / sum(train2_table$freq)
train2_table
```

The Confusion Matrix:
``` {r ConfusMatrix}
ConfusMatrix <- confusionMatrix(train2_pred, train2$classe)
ConfusMatrix
```


##Interpreting the model:

Classification Accuracy:
```{r ClassAccur}
sum(diag(ConfusMatrix$table))/sum(ConfusMatrix$table)
```

This simply shows an extremely accurate model.  This is a great model.

Importance Values:
One set of metrics I chose to analyze are the importance values included with the random forest algorithm.  A simple explanation would be that these numbers quantatize some amount of a column's utility towards the predictive power of this Random Forest Model. If the top variable from the model is dropped, the model's predictive power will greatly reduce. On the other hand, if one of the bottom variables is dropped, there would not be much of an impact on predictive power of the model.

Now the two charts give two different measures of the same thing(they are somewhat different in interpretation), but what I have observed is that they generally give same results unless there are some categorical variables with many levels.

```{r varImpPlot}
varImpPlot(RFModel,  main="Top 25 Variable Importance")
```


Another analysis I performed was using the varImp function to effectively obtain the same quantization of importance for the model. echo = FALSE for this because of how long the code is.
```{r plotly1, echo = FALSE}
RFImportance <- varImp(RFModel, scale=FALSE)
RFImportance$Category <- row.names(RFImportance)
RFImportanceMelt <- melt(RFImportance, "Category")
RFImportance <- RFImportance[order(RFImportance$Category),]
RFImportanceMelt <- RFImportanceMelt[order(RFImportanceMelt$Category),]

f1 <- list(
  family = "Arial, sans-serif",
  size = 14,
  color = "black"
)
f2 <- list(
  family = "Old Standard TT, serif",
  size = 9,
  color = "black"
)
a <- list(
  title = "Names of the Columns of the Training Dataset after Tidying",
  titlefont = f1,
  showticklabels = TRUE,
  tickangle = 90,
  tickfont = f2
)

b <- list(
  title = "Value of Importance determined by the Random Forest Algorithm",
  titlefont = f1,
  showticklabels = TRUE,
  tickfont = f2
)

m = list(
  l = 50,
  r = 40,
  b = 110,
  t = 40,
  pad = 0
) 

plot_ly(data = RFImportanceMelt, x = ~Category, y = ~value, 
        type = 'scatter', mode = 'lines', color = ~variable)  %>%
  layout(xaxis = a, yaxis = b, 
    autosize = T, margin = m, title = "Values of Importance for Each Column \n Imagined as a Line")
```


This graph looks at how the error for each category reduces as more trees are added to the model.  The log of the error is taken so that changes in the error can be more easily noticed.
```{r ErrorPlot}
 plot(RFModel, log="y")
```

Finally, I will analyze the model using the partial plot to see how each column contributes to the model, looking only at the top 6 columns.
```{r partialPlots}
##PartialPlots
 imp <- importance(RFModel)
 impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)][1:6]
 dev.new(width=7, height=7)
 par(mar=c(1.5, 1.5, 1.2, 1.2))
 op <- par(mfrow=c(3, 2))
 for (i in seq_along(impvar)) {
   partialPlot(RFModel, train1, impvar[i], xlab=impvar[i],
               main=paste("Partial Dependence on", impvar[i])
)
 }
 par(op)
```


##Verifying the results:
```{r verify}
test_pred <- predict(RFModel, newdata=test)
test_table <- count(test_pred)
test_table$Percent <- test_table$freq / sum(test_table$freq)
test_table
```

This simply show how the model might be applied to a future set of data.  In this case, the test dataset, and the results are statistically similar to the training data.


#Thank you for your time.
